{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering and Model Optimization\n",
    "\n",
    "### Created by @YankunQiu and @JunLuo\n",
    "#### 22 Dec 2017 - 10 Jan 2018"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Purpose of this document\n",
    "\n",
    "This document shows the data mining and machine learning process of the LanceGuard project, and the knowledge we found which is important for solving the problems. This document mainly focuses on the work done from 22 Dec 2017 to 10 Jan 2018.\n",
    "\n",
    "The document will be structured into three sections:\n",
    "\n",
    "1. Research\n",
    "\n",
    "2. Implementation\n",
    "\n",
    "3. Evaluation and Analysis\n",
    "\n",
    "4. Next Steps\n",
    "\n",
    "5. References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Research"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1) Time-series features: \n",
    "\n",
    "One of the main ideas in this project is to transfer the unfamiliar problem into a familiar problem. Since the motion detection of lances has not appeared in any research, it is difficult to come up with a brand new way of doing this. Luckily, inspired by smart wristband, a popular wearable device such as Fitbit and Mi Band, the problem can be transferred to activity classification. Mannini and Sabatini's paper introduced experiments using different features in this problem, as well as their results (Mannini, A. and Sabatini, A.M., 2010). \n",
    "![Previous Results](https://image.ibb.co/ian2HG/previous_results.png)\n",
    "\n",
    "\n",
    "The paper also shows the conceptual scheme of a generic classification system with supervised learning.\n",
    "\n",
    " \n",
    " ![Flow Chart](https://image.ibb.co/jUtpWb/flow_chart.png)\n",
    "\n",
    "\n",
    "The use of wavelet coefficient (Sekine, M., Tamura, T., Togawa, T. and Fukui, Y., 2000) is in several papers, which is worth researching in the future.\n",
    "\n",
    "\n",
    "### (2) Feature Transformation\n",
    "\n",
    "For this part, we aim to determine the effect of IMU position on the model's performance. After identifying the influence made by changing the IMU position, we also hope to apply some feature transformation technics to minimize the influence and get better results. \n",
    "\n",
    "To achieve this, we first did some simple analysis about the given data set. The detail is shown in the table below.\n",
    "\n",
    "<img src=\"https://image.ibb.co/f816mm/chart.png\">\n",
    "\n",
    "To see the differences between these data, we present the data in a more obvious way, by calculating the mean, variance, min and max of the acceleration along x, y and z axises.\n",
    "\n",
    "<img src=\"http://image.ibb.co/ch0z6m/mean.png\">\n",
    "\n",
    "<img src=\"http://image.ibb.co/ni5oz6/variance.png\">\n",
    "\n",
    "<img src=\"http://image.ibb.co/kHRve6/max.png\">\n",
    "\n",
    "<img src=\"http://image.ibb.co/eUfz6m/min.png\">\n",
    "\n",
    "\n",
    "#### 2.1 Identify the Vertical Direction\n",
    "\n",
    "Initially, we set the y axis along the direction of the lance, but the performance of the model is not really good. By observing the data we have, we found that the accelerations along the y and z axises are quite similar, while the accelerations along the x axis are much larger. And we also did some research, it says that the acceleration is usually greater than those one other two directions cause the existence of gravity. So we think the x axis may be the vertical direction, and our following work is based on this assumption.\n",
    "\n",
    "#### 2.2 Axis Scaling\n",
    "\n",
    "Normalization can have a range of meanings. The simplest way of normalization may be adjusting values measured on different scales to the same scale, often prior to averaging.\n",
    "\n",
    "The lance would move back and forth giving a fixed pointed. The acceleration along y and z axises would change if we change the position of the sensor. And after analyzing, we think that the accelerations along y and z axises would increase/decrease with some common factor by changing the position of the sensor. To apply the feature normalization in this task, we try to scale the accelarations along the y and z axises. We think this can minimize the influence of different sensor positions.\n",
    "\n",
    "$$y^\\prime = \\frac{y}{max(y, z)}, z^\\prime = \\frac{z}{max(y, z)}$$\n",
    "\n",
    "#### 2.3 Axis Rotation\n",
    "\n",
    "Adding some noise to the training set is a very common technic to make the model generalize, which can prevent the model from overfitting. Basically, there are two reasons. \n",
    "\n",
    "1. If the training set is too small or the model is too complicated, the model tends to overfit the data. By introducing some noise into the data set, we can build a larger data set to make the model fit the new data better.\n",
    "\n",
    "2. An intuitive way to illustrate is, the testing data is different from the training data. We can treat this difference as “noise”. Thus, if the trained model is forced to fit noised training data, the model probably fit the testing data as well.\n",
    "\n",
    "When we worked on this task, we found that even if the sensors are in the same position of the lance, which means the distances between them and the fixed point are the same, the acceleration along y and z axises can be different if we rotate the sensors with some angle. But the acceleration on the vertical direction remains unchanged. The solution we came up with is adding some noise to the training set. For each item in the training set, we rotate the axises of y and z with a mathematical method. This may prevent the model from overfitting the training data.\n",
    "\n",
    "### (3) Hyper-parameter Optimization\n",
    "\n",
    "Hyper-parameter optimization is an essential and time-consuming task for almost any machine learning algorithms. The aim of it is to find the best hyper-parameters for the model and improve its performance on testing data. \n",
    "\n",
    "There are three main methods for hyper-parameter optimization: Exhaustive Search, Grid Search and Random Search(Bergstra, J. and Bengio, Y., 2012).\n",
    "\n",
    "![Random Search vs Grid Search](https://image.ibb.co/mX7mBb/random_search.png)\n",
    "\n",
    "Grid search is a way similar to exhaustive search, just more coarse-grained. It assumes every hyper-parameter has the same importance.\n",
    "\n",
    "However, Random Search assumes each hyper-parameter has a probability to be more important than others. It maintains a weighted parameter list.\n",
    "\n",
    "Another thing is when different trails have nearly optimal validation means, then it is not clear which test score to report, and a slightly different choice of hyper-parameter could have yielded a different test error.(Bergstra, J. and Bengio, Y., 2012) Random Search deals with this problem by assigning probabilities to parameter sets.\n",
    "\n",
    "### (4) DNN\n",
    "\n",
    "Since we are going to use the most popular python deep-learning library Tensorflow, the research on DNN in this period is mostly based on the official document of Tensorflow. In its website, there is a paragraph of the introduction:\n",
    "\n",
    "\"TensorFlow™ is an open source software library for numerical computation using data flow graphs. Nodes in the graph represent mathematical operations, while the graph edges represent the multidimensional data arrays (tensors) communicated between them. The flexible architecture allows you to deploy computation to one or more CPUs or GPUs in a desktop, server, or mobile device with a single API. TensorFlow was originally developed by researchers and engineers working on the Google Brain Team within Google's Machine Intelligence research organization for the purposes of conducting machine learning and deep neural networks research, but the system is general enough to be applicable in a wide variety of other domains as well.\"\n",
    "\n",
    "![TensorFlow](https://avatars0.githubusercontent.com/u/15658638?s=200&v=4)\n",
    "\n",
    "The concept of Deep Neural Network has been existing for decades. However, it is only becoming popular because of the boosting of computational power in the recent three years. The structure of the neural network can be very deep, but according to a Quora answer, most problems in the world can be solved using one or two layers of neural network. The idea is that we need to keep the model simple and well performing.\n",
    "\n",
    "Also, different structure of DNN has different effects on different kinds of data. \n",
    "\n",
    "The structures we can try in this project are: \n",
    "\n",
    "1. CNN(Convolutional Neural Network)\n",
    "2. RNN(Recurring Neural Network)\n",
    "3. LSTM(Long-short Term Memory)\n",
    "\n",
    "\n",
    "### (5) RNN, LSTM and CNN\n",
    "\n",
    "RNN can find the relationship between time t and t-1 and t-2. The neurons in it flow around each other. See videos of Siraj(https://www.youtube.com/watch?v=cdLUzrjnlr4). And this tutorial introduced the Tensorflow version of RNN (https://github.com/llSourcell/How-to-Use-Tensorflow-for-Time-Series-Live-/blob/master/demo_full_notes.ipynb).\n",
    "\n",
    "LSTM(Long Short Term Memory) neural network can learn what to remember and what to forget in the sequence data. This tutorial introduces the structure of LSTM and how to implement it in a non-library way.(https://github.com/llSourcell/LSTM_Networks/blob/master/LSTM%20Demo.ipynb)\n",
    "![LSTM Structure](https://camo.githubusercontent.com/284f12768a57940bbd21c5e9746e5d4bf6f22fea/68747470733a2f2f7777772e7265736561726368676174652e6e65742f70726f66696c652f4d6f6873656e5f46617979617a2f7075626c69636174696f6e2f3330363337373037322f6669677572652f666967322f41533a33393830383238343931363533313440313437313932313735353538302f4669672d322d416e2d6578616d706c652d6f662d612d62617369632d4c53544d2d63656c6c2d6c6566742d616e642d612d62617369632d524e4e2d63656c6c2d72696768742d4669677572652e70706d)\n",
    "\n",
    "CNN(Convolutional Neural network) can be used in this time-series problem(Yang, J., Nguyen, M.N., San, P.P., Li, X. and Krishnaswamy, S., 2015). The structure of the network is shown in the picture below. The difficulty of using CNN is that in the paper, the number of features is nearly 50. We only have about 7 features in total. It might have some effect on our precision. But this method is totally worth trying.\n",
    "\n",
    "![CNN](https://preview.ibb.co/j0oWEw/cnn.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1) Algorithm Independent Tasks (Both)\n",
    "#### a. Feature extraction: Time series(Jun)\n",
    "\n",
    "While doing the feature extraction, the method we mainly use is \"Rolling Window\". This is one of the most popular ways of dealing with time-series data. The basic operation of \"Rolling window\" is to take an n-data-points long data sequence, and extract features from this window of data points. \n",
    "\n",
    "Using the python library Pandas, this can be done more conveniently.\n",
    "![Pandas](https://pandas.pydata.org/_static/pandas_logo.png)\n",
    "\n",
    "After adding the time series features, our feature space expands from 3 to 9.\n",
    "\n",
    "    ['timeStamp',\n",
    "\n",
    "    'x', 'y', 'z',\n",
    "    \n",
    "    'Rolling_Mean_x','Rolling_Mean_y','Rolling_Mean_z',\n",
    "    \n",
    "    'Rolling_Std_x','Rolling_Std_y','Rolling_Std_z',\n",
    "    \n",
    "    'label']\n",
    " \n",
    "    [1510837962239.0, \n",
    "    \n",
    "    -0.9301766753196716, -0.19591960310935974, -0.14742934703826904, \n",
    "    \n",
    "    -0.9847284739358084, -0.11343124378173212, -0.023986388131244374, \n",
    "    \n",
    "    0.09911378051724326, 0.14364807808884125, 0.15149487064158973,\n",
    "    \n",
    "    0.0]\n",
    "    \n",
    "![9features](https://preview.ibb.co/cz1uMm/9features.png)\n",
    "\n",
    "\n",
    " Notice that we did not add the 'w' to the feature space. The next steps can investigate the effect of 'w'(rotation angle of the device) and add it as a feature if needed. \n",
    "\n",
    "Note that this method of feature selection is called 'Heuristic Based Feature Selection'. It is based on the researchers' knowledge about the data. This is the typical way of selecting features. \n",
    "\n",
    "However, when using the deep neural network, a better way is to just use raw data as features. The hidden units in the network can automatically learn the functions to summarize the most effective features based on the raw data. This is also a point we want to further study about.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### b. Feature Transformation(Yankun)\n",
    "\n",
    "##### b.1 Axises rotation\n",
    "\n",
    "<img src=\"http://image.ibb.co/ipmcmb/IMG_6214.jpg\">\n",
    "\n",
    "The picture above shows how we rotate the axis in a mathematical way. \n",
    "In different groups of experiments, the directions of the y and z axises can be different, depending on how to install the sensors. When processing the training data, we rotate the y, z axises with the angle from 1 to 359. This way, we can introduce some noise into the data set. Besides, we get a much larger training set because we add a lot of new data items.\n",
    "\n",
    "##### b.2 Feature scaling\n",
    "\n",
    "<img src=\"http://image.ibb.co/bNg0cR/Wechat_IMG1.jpg\">\n",
    "\n",
    "The picture above illustrates the relationship between the accelerations along the y, z axises and the distance between the sensors and the fixed point. As long as the sensors are installed on the same lance, they should have the same angular velocities, and that's the idea behind scaling the axises. Scaling the axises is just like moving the sensor to a different position. By scaling the axises, we hope to minimize the influence made by moving the sensors.\n",
    "\n",
    "\n",
    "#### c. GPU support on Windows 10: NVIDIA GEFORCE 750M(Jun)\n",
    "\n",
    "We run the same task on the CPU, GPU and both. The task is training and predicting in a 3-layer deep neural network, 1000 epoch. The result of the running time is shown below:\n",
    "\n",
    "CPU: 00:01:20\n",
    "\n",
    "GPU: 00:01:15\n",
    "\n",
    "CPU+GPU: 00:01:36\n",
    "\n",
    "\n",
    "Seems it does not speed up much of the process. Only 5 seconds are saved in the task.\n",
    "\n",
    "We believe the reasons why it has not improve much are: \n",
    "\n",
    "1. NVIDIA GEFORCE 750M is a GPU with the computation capability of 3, according to NVIDIA website(https://developer.nvidia.com/cuda-gpus). \n",
    "\n",
    "2. The performance can be improved using suggestions from Tensorflow's document. Some improvements can be used to implement in the future code based on the document. (https://www.tensorflow.org/versions/r1.1/performance/performance_guide)\n",
    "\n",
    "3. The combination of CPU and GPU may cause some delay by the memory exchange between them.\n",
    "\n",
    "\n",
    "The local GPU support configuration helps us understand the usage of GPU on Tensorflow (how to deploy a new GPU, how to write code that is suitable for GPU computation, how to run computation tasks on single/multiple CPUs and GPUs) and able to work on the remote Teamviewer workstation using NVIDIA 1050 Ti.\n",
    "\n",
    "#### d. Data Warehouse\n",
    "\n",
    "During the analysis, we need a convenient way to retrieve data. For example, I want to get the data files which are using the I-1 tip. I also want to know all the other attributes of the files such as gas flow, mount types and so on.\n",
    "\n",
    "We built a SQL database for storing the attributes of the data. After importing the .sql file into a database, we can use SQL language to query for data we need. The link to the SQL file is in the comment of this page: https://confluence.myoutotec.com/display/AUS/Lance+Motion+Trial+Record\n",
    "\n",
    "\n",
    "### (2) Random Forest Model(Jun)\n",
    "#### a. Parameter Tuning\n",
    "There are 2 main hyper-parameters which are needed to be optimized:\n",
    "\n",
    "1. Length of the window\n",
    "2. Number of the trees in the Random Forest\n",
    "\n",
    "There are certain rules for these parameters. Since we only have 9 features, we do not need too many trees in the forest. Testing the number of trees from 1 to 30 is good enough.\n",
    "\n",
    "We should do a grid search for tuning the length of the window. Since the time between any two adjacent data points is 32ms, we do not need the window to cover the time period that is too long. 1000 data points would be the longest window we will test.\n",
    "\n",
    "The result of tuning Random Forest is shown in the pictures below. \n",
    "\n",
    "##### Tuning Number of Trees:\n",
    "\n",
    "![Tuning Number of Trees](https://image.ibb.co/npkCmb/tree_num_tune.png)\n",
    "\n",
    "The x-axis shows the number of trees, and the y-axis shows the accuracy of the algorithm. The algorithm converges to the 10 trees.\n",
    "\n",
    "##### Tuning Length of Window:\n",
    "![Tuning Length of Window](https://image.ibb.co/d214XG/window_tune.png)\n",
    "\n",
    "The x-axis shows the length of the window, and the y-axis shows the accuracy of the algorithm. The algorithm converges to the length of 100.\n",
    "\n",
    "\n",
    "#### b. Computational Parallelization\n",
    "\n",
    "Random Forest is an ensemble learning algorithm, which is able to be parallelized. All trees can be trained at the same time. However, the python library we use in Scikit-learn does not support GPU computation. But it still provides a way to speed up the algorithm. The official document states this feature(http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html):\n",
    "\n",
    "\"n_jobs : integer, optional (default=1)\n",
    "\n",
    "The number of jobs to run in parallel for both fits and predict. If -1, then the number of jobs is set to the number of cores.\"\n",
    "\n",
    "\n",
    "Based on the number of cores in a training machine, we can change the parameter of 'n_jobs' to parallel the training process to make it more efficient.   \n",
    "\n",
    "The picture below shows the effectiveness of using multi-core computation on HP ELITEONE computer.(i7-6700 CPU @ 3.40GHz, 4 Cores, 16G RAM)\n",
    "\n",
    "![cpu100%](https://preview.ibb.co/eM7vSR/cpu100.png)\n",
    "\n",
    "### (3) Artifitial Neural Network(Jun)\n",
    "#### a. Tensorflow\n",
    "\n",
    "As mentioned above, we use Tensorflow as our deep learning library.\n",
    "\n",
    "#### b. Softmax/Relu Regression & Choosing Activation Functions\n",
    "\n",
    "![multiple layers](https://cdn-images-1.medium.com/max/479/1*QVIyc5HnGDWTNX3m-nIm9w.png)\n",
    "\n",
    "The image above shows the structure of the neural network. \n",
    "\n",
    "The first layer of Softmax/Relu Regression Neural Network is the input layer. The number of neurons is the same as the number of features.\n",
    "\n",
    "The second layer is the hidden layer. In this 1-hidden layer model, the number of hidden unit in this layer is equal to the number of class.\n",
    "\n",
    "Between the seccond and third layer, there is  a Softmax/Relu activation function. It supports the model to fit a non-linear model.\n",
    "\n",
    "The third layer is the output layer. The number of neurons is the same as the number of class.\n",
    "\n",
    "Note that the output layer uses one-hot matrix. For example, 100 represents class 1; 010 represents class 2; 001 represents class 3. This prevents the numerical relationship in the classes.\n",
    "\n",
    "This implementation is a foundation of the other deeper neural network.\n",
    "\n",
    "This requires an algorithm to covert labels into \"one-hot vectors\". We use python Numpy library to convert labels into one-hot vectors.\n",
    "\n",
    "We use Gradient Descent as an optimizer to train the neural networks.\n",
    "\n",
    "#### c. Deep Neural Network (Layer and Hidden Units)\n",
    "\n",
    "We then try the neural network with more layers: 2 to 3 hidden layers.\n",
    "\n",
    "The structure of the network remains the same(only added hidden layers, no changes to input and output layers)\n",
    "\n",
    "![dnn](https://qph.ec.quoracdn.net/main-qimg-7c35987ad55173b3b76214b9112830ff.webp)\n",
    "\n",
    "\n",
    "#### d. Parameter Tuning\n",
    "\n",
    "In section b above, if there is only one hidden layer in the neural network, then there are not many parameters to adjust. The size of input neurons and output neurons are fixed.\n",
    "\n",
    "In section c, the parameters can be tuned are the number of layers in the network and number of neurons in each layer. We restrict the number of layers to 2 or 3. Based on some rule of thumb from the previous research, the number of each layer should not exceed 2 times of the number of input. \n",
    "\n",
    "Since we have 9 features in the data, we can tune the parameters in the following ways:\n",
    "\n",
    "1. 2 layers: hidden layer 1(L1 = 2-18 hidden units), hidden layer 2(same as output layer)\n",
    "2. 3 layers: hidden layer 1(L1 = 2-18 hidden units), hidden layer 2(2-L1*2 hidden units), hidden layer 3(same as output layer)\n",
    "\n",
    "This tuning period will be very time-consuming. It needs a computer to run for the whole night."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Evaluation and Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(We mainly focus on Random Forest)\n",
    "\n",
    "### (1) Effect of Feature Engineering(Both)\n",
    "\n",
    "To compare the model's performance when using different features, we design a series of experiments. There are 16 groups in total, the chart below shows the detail of the experiments.\n",
    "\n",
    "<img src=\"http://image.ibb.co/fK5h7R/experiment.png\">\n",
    "\n",
    "In the experiments, we use the data from trial 0a and 0b as the training set, data from trial 1a, 1e, 4a and 3f as testing set when doing binary classification. For multi-class classification, we use data from trial 1b, 4e, 0b, 4b, and 4c as the training set, data from trail 4a, 4f, 3f, 0c and 2b as the testing set.\n",
    "\n",
    "#### a. Time-series Feature vs Non-Time-series Feature\n",
    "\n",
    "#### b. Scaling vs Non-Scaling\n",
    "\n",
    "From the previous experiment, we can compare the results in pairs, the detail is showed in the picture below.\n",
    "\n",
    "The grey/blue bars represent the experiments with scaling and the yellow bars represents the experiments with non-scaling features.\n",
    "\n",
    "Overall, the scaling experiments have better performance over the non-scaling ones.\n",
    "\n",
    "<img src=\"http://image.ibb.co/eppnrm/recall_scaling.png\">\n",
    "\n",
    "<img src=\"http://image.ibb.co/du8oWm/precision_in_groups.png\">\n",
    "\n",
    "#### c. Rotation vs Non-Rotation\n",
    "\n",
    "The reason we rotate the axis is adding some noise into the data set, and it helps to prevent from overfitting and to build the large dataset. However, in the previous experiments, the sensors are installed without rotation, which means, overfitting may give better results on even testing data. To see if rotate the axises could really improve the performance of the model on more general data set, we did another test.\n",
    "\n",
    "Training Data | Testing Data | Feature | Scaling | Rotation | Precision | Recall\n",
    "---- | --- | --- | --- | --- | --- | ---\n",
    "4h, 4g | 4i | Full | Yes | No | 1.00 | 0.58\n",
    "4h, 4g |  4i | Full | Yes | Yes | 1.00 | 0.97\n",
    "\n",
    "Testing the model on the new data set, we can see that the performance is improved significantly. Because the sensor in trail 4i is rotated with 90 degree. By rotating the axises, we can make the model more generalize.\n",
    "\n",
    "#### d. Effect of Parameter Tuning in Random Forest Model\n",
    "\n",
    "See last section.\n",
    "\n",
    "\n",
    "### (2) Result After Feature Engineering and Parameter Tuning(Yankun)\n",
    "\n",
    "#### a. Binary Classification\n",
    "\n",
    "![bin_Result_vs_last_Time](https://image.ibb.co/cDnUnR/bin_Result_vs_last_Time.png)\n",
    "\n",
    "Using the data of the same as the last presentation, we did greatly improved the accuracy of our model.\n",
    "\n",
    "![improvement](https://image.ibb.co/kQZ9nR/binary_Improvement.png)\n",
    "\n",
    "This is the held-out result. 80% triaining, 20% testing. The training and testing data come from the same files.\n",
    "\n",
    "After we get more and more data, we start to try more complicated experiments. Because of the difference of the data, we assume that the result of the experiments will not be good before the experiment.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### b. Multi-class Clasification\n",
    "\n",
    "The data of Multi-class classification is: for each tip, we use one file as training and another file as testing. We tried to classify 5 tips in the experiments. The training and testing files are chosen as random as possible. We want to test whether this model is now powerful enough to classify tips using insufficient training data.\n",
    "\n",
    "As shown in the results, the multi-class classifier failed to classify well. It shows a very bad performance, but it is the same as our expectation. The new coming data has more complicated variables such as gas flow, NGIMU mount position, lance movement, different Bath Start Depth, IMU orientation and so on. All these affect the training performance.\n",
    "\n",
    "As we actually perform well using held-out data, the next step is to generalize the model. Scaling, rotating are both generalization processes we have done, and we need more other methods. \n",
    "\n",
    "\n",
    "### (3) Result of Deep Neural Network\n",
    "\n",
    "#### a. 1 layer vs 2 layer vs 3 layer: Relu/Softmax Regression\n",
    "\n",
    " 1 Layer: Training 0.91, Testing 0.55\n",
    "\n",
    " 2 Layers:\n",
    "\n",
    "![hiddenVSaccuracy](https://image.ibb.co/mwzxP6/hidden_VSaccuracy.png)\n",
    "\n",
    "3 Layers: Result is almost the same as 2 Layers.\n",
    "\n",
    "#### b. Why?\n",
    "The model is ineffective. The parameter tuning method is the typical way of tuning this kind of simple structured deep neural network. If it does not work well, then it indicates that the model is not sufficient for this problem.\n",
    "\n",
    "The features might not be good enough for the neural network model. The network should be able to read raw data and find out the features by itself.\n",
    "\n",
    "The structure of the DNN can be changed. As introduced in section 1, CNN, RNN, and LSTM are models with more complicated structures. They are theoretically working well in this kind of problem(according to some).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.5 Question: What data should we use for Training and Testing?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Next Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a. Deep Learning Cont. (may use 350*3 features x1,x2...x350, y1, y2, y350)\n",
    "#### b. More data, more classes\n",
    "#### c. Experiment -> Real industrial condition\n",
    "#### d. PCA\n",
    "#### e. HMM\n",
    "#### f. Tag the process features(drain, lift up...), add the tilt angle data into feature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Final Thoughts in This Period of Work\n",
    "#####  a. Research is very important. Typically there are two ways of research: (1) do reseach in advance and implement based on that. (2) based on a certain problem in the development/experiment process, do further research to dig deeper into the problem. \n",
    "\n",
    "##### b. The experiments might take longer time than development. Analysis of the experiment result help us understand which way we should go next.\n",
    "\n",
    "##### c. There are many attributes that can affect the performance of the algorithm. The management of attributes is a important part in development and doing experiments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[1] Bergstra, J. and Bengio, Y., 2012. Random search for hyper-parameter optimization. Journal of Machine Learning Research, 13(Feb), pp.281-305.\n",
    "\n",
    "[2] Mannini, A. and Sabatini, A.M., 2010. Machine learning methods for classifying human physical activity from on-body accelerometers. Sensors, 10(2), pp.1154-1175.\n",
    "\n",
    "[3] Sekine, M., Tamura, T., Togawa, T. and Fukui, Y., 2000. Classification of waist-acceleration signals in a continuous walking record. Medical engineering & physics, 22(4), pp.285-291.\n",
    "\n",
    "[4] Yang, J., Nguyen, M.N., San, P.P., Li, X. and Krishnaswamy, S., 2015, July. Deep Convolutional Neural Networks on Multichannel Time Series for Human Activity Recognition. In IJCAI (pp. 3995-4001).\n",
    "\n",
    "[5] Yao, S., Hu, S., Zhao, Y., Zhang, A. and Abdelzaher, T., 2017, April. Deepsense: A unified deep learning framework for time-series mobile sensing data processing. In Proceedings of the 26th International Conference on World Wide Web (pp. 351-360). International World Wide Web Conferences Steering Committee.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
